{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "108dcff2",
   "metadata": {},
   "source": [
    "### **Librerias necesarias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de0a19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, date\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import psycopg2\n",
    "import json\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e2f72a",
   "metadata": {},
   "source": [
    " ## **<u>Conectar a la API</u>** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8191f392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el archivo de credenciales\n",
    "with open('credentials.json') as file:\n",
    "    credentials = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361da84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parametros de la busqueda\n",
    "\n",
    "query = '''ucrania OR Ukraine OR Ukraine OR Ucraina'''#palabras clave para efectuar la busqueda en API\n",
    "\n",
    "today = datetime.today()\n",
    "yesterday = today - timedelta(days=1) # Obtener la fecha del dia de ayer\n",
    "from_date = yesterday.strftime('%Y-%m-%d')\n",
    "results_limit = 100 \n",
    "api_key = credentials['api_key']\n",
    "languages = ['es', 'en', 'fr', 'it']\n",
    "\n",
    "# Funcion para realizar la conexion a la API\n",
    "def get_articles(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print('Error al conectarse a la API:', e)\n",
    "        return []\n",
    "    else:\n",
    "        data = response.json()\n",
    "        articles = data['articles']\n",
    "        return articles\n",
    "\n",
    "# Crear una lista de diccionarios con los datos de cada artículo.\n",
    "articles_list = []\n",
    "for lang in languages:\n",
    "    url = f\"https://newsapi.org/v2/everything?q={query}&language={lang}&from={from_date}&pageSize={results_limit}&apiKey={api_key}\"\n",
    "    articles = get_articles(url)\n",
    "    for article in articles:\n",
    "        article_data = {\n",
    "            'author': article['author'],\n",
    "            'title': article['title'],\n",
    "            'description': article['description'],\n",
    "            'url': article['url'],\n",
    "            'publishedAt': article['publishedAt'],\n",
    "            'source': article['source']['name'],\n",
    "            'language': lang\n",
    "                        }\n",
    "        articles_list.append(article_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1fb665",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convertir la lista de diccionarios en un DataFrame\n",
    "df = pd.DataFrame(articles_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cd70c4",
   "metadata": {},
   "source": [
    "### **Guardar en un .csv los datos de la API, sin procesar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adcb055",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se guardara un .csv distinto por dia.\n",
    "current_folder = os.getcwd()\n",
    "destination_folder = os.path.join(current_folder, 'Raw_csvs')\n",
    "yesterday = date.today() - timedelta(days=1)\n",
    "date_str = yesterday.strftime(\"%Y-%m-%d\")\n",
    "file_name = os.path.join(destination_folder, f\"news {date_str}.csv\")\n",
    "df.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42883833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Especifica la ruta del archivo CSV\n",
    "ruta_archivo = 'C:\\\\Users\\\\juanm\\\\OneDrive\\\\Escritorio\\\\pythonProject\\\\Data. Eng --Coderhouse\\\\Raw_csvs\\\\news 2023-05-28.csv'\n",
    "\n",
    "# Lee el archivo CSV en un DataFrame\n",
    "df = pd.read_csv(ruta_archivo)\n",
    "\n",
    "# Haz uso del DataFrame df para realizar las operaciones necesarias\n",
    "# ...\n",
    "\n",
    "# Imprime el DataFrame para verificar los datos cargados\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c943960",
   "metadata": {},
   "source": [
    "## **<u>Limpiar los datos</u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b3da8c",
   "metadata": {},
   "source": [
    "#### ***Declarar algunas variables y funciones***\n",
    "\n",
    "Creo 2 funciones para limpiar los textos, quitando simbolos, comas, tildes, parentesis, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d8900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base para reordenar las columnas\n",
    "column_order = ['source', 'title', 'description','author',  'publishedAt','language', 'url']\n",
    "\n",
    "def eliminar_tildes(texto):\n",
    "    caracteres_con_tilde = 'áéíóúÁÉÍÓÚ'\n",
    "    caracteres_sin_tilde = 'aeiouAEIOU'\n",
    "    reemplazar_map = str.maketrans(caracteres_con_tilde, caracteres_sin_tilde)\n",
    "    texto_sin_tilde = unidecode(texto.translate(reemplazar_map))\n",
    "    return texto_sin_tilde\n",
    "\n",
    "def limpiar_celdas(dataframe, columnas):\n",
    "    patron = r\"[^a-zA-ZáéíóúÁÉÍÓÚüÜñÑ\\d\\s/]\"\n",
    "    for columna in columnas:\n",
    "        dataframe[columna] = (\n",
    "            dataframe[columna]\n",
    "            .apply(lambda x: re.sub(patron, '', str(x)))\n",
    "            .apply(eliminar_tildes)\n",
    "        )\n",
    "    return dataframe\n",
    "\n",
    "columnas_limpiar = ['source', 'title', 'description', 'author']\n",
    "\n",
    "#Esta funcion sirve para casos especificos. Borra lo que haya entre parentesis en una columna en particular\n",
    "def eliminar_parentesis(columna):\n",
    "    patron = r'\\((.*?)\\)'  # Patrón de expresión regular para encontrar el texto entre paréntesis\n",
    "    columna = columna.apply(lambda x: re.sub(patron, '', x)).str.strip()\n",
    "    # Eliminar el texto entre paréntesis y eliminar espacios en blanco\n",
    "    return columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f3cd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_cleaned = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c56cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_cleaned.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94122631",
   "metadata": {},
   "source": [
    "#### **Limpieza general del df**\n",
    "Pasos a realizar:\n",
    "\n",
    "- Eliminar duplicados (hay muchas notas que estan duplicadas pero difieren en el url. Tener eso en cuenta)\n",
    "- Pasar palabras a lowercase, convertir las siglas de la columna lenguaje, reordenar las columnas\n",
    "- Reemplazar valores vacios por Nan (despues se rellenan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5455e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_cleaned = news_cleaned.drop_duplicates(subset=['source', 'title', 'description', 'author', 'publishedAt', 'language'])\n",
    "news_cleaned = news_cleaned.apply(lambda x: x.astype(str).str.lower())\n",
    "news_cleaned = news_cleaned.replace('', np.nan)\n",
    "news_cleaned = news_cleaned.reindex(columns=column_order)\n",
    "siglas_a_idiomas = {\n",
    "    'en': 'english',\n",
    "    'fr': 'french',\n",
    "    'it': 'italian',\n",
    "    'es': 'spanish'\n",
    "}\n",
    "news_cleaned['language'] = news_cleaned['language'].map(siglas_a_idiomas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960f6b1a",
   "metadata": {},
   "source": [
    "### **Limpieza especifica por columna**\n",
    "\n",
    "En este orden: \n",
    "- Elimino parentesis y contenido de celdas en columna source.\n",
    "- Reemplazo Nan values, y 'none' en columnas 'description' y 'author'. \n",
    "- En ambos casos, se reemplaza por lo que dice en la columna 'source'\n",
    "- Muchos autores figuran asi: 'la nacion (Carlos pagni)'. Dejo solo los nombres que figuran entre parentesis.\n",
    "- Ajusto manualmente unos pocos casos, usando replace\n",
    "- Ejecuto la funcion limpiar_celdas en columnas seleccionadas\n",
    "- Modifico la columna publishedAt, para que quede el formato fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b1c27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_cleaned['source'] = eliminar_parentesis(news_cleaned['source'])\n",
    "news_cleaned['description'] = news_cleaned['description'].fillna('none')\n",
    "news_cleaned['author'] = news_cleaned['author'].fillna(news_cleaned['source'])\n",
    "mask = news_cleaned['author'].eq('none')\n",
    "news_cleaned['author'] = news_cleaned['author'].where(~mask, news_cleaned['source'])\n",
    "mask = news_cleaned['author'].str.contains(r'\\([^)]+\\)')\n",
    "news_cleaned.loc[mask, 'author'] = news_cleaned.loc[mask, 'author'].str.extract(r'\\(([^)]+)\\)', expand=False)\n",
    "news_cleaned['author'] = (\n",
    "    news_cleaned['author']\n",
    "    .str.replace('rt en español\\n', 'rt en español')\n",
    "    .str.replace('https//www.facebook.com/bbcnews', 'bbcnews')\n",
    "    .str.replace('rt en español , rt en español', 'rt en español'))\n",
    "news_cleaned = limpiar_celdas(news_cleaned, columnas_limpiar)\n",
    "news_cleaned['publishedAt'] = pd.to_datetime(news_cleaned['publishedAt'])\n",
    "news_cleaned['publishedAt'] = news_cleaned['publishedAt'].dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78290902",
   "metadata": {},
   "source": [
    "## **<u>Conectar con Amazon Redshift</u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e04517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar conexion a Redshift mediante psycopg2\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "    host='data-engineer-cluster.cyhh5bfevlmn.us-east-1.redshift.amazonaws.com',\n",
    "    dbname=credentials['db_username'],\n",
    "    user=credentials['db_password'],\n",
    "    password='qjA21aB81Y',\n",
    "    port='5439'\n",
    ")\n",
    "    print(\"Conexion con Redshift exitosa\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Error al conectar con Redshift\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb2474c",
   "metadata": {},
   "source": [
    "## **<u>Crear la tabla en Redshift</u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b1360b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un cursor para ejecutar sentencias SQL\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62449eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar la tabla si ya existe. Ejecutar solo en caso de error\n",
    "#cur.execute('DROP TABLE IF EXISTS news_articles')\n",
    "\n",
    "# Crear una tabla en Redshift, en caso de que no exista\n",
    "table_name = 'news_articles'\n",
    "cur.execute(f'''\n",
    "CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "    id INT IDENTITY(1, 1),\n",
    "    source VARCHAR(255),\n",
    "    title VARCHAR(1000),\n",
    "    description VARCHAR(1000),\n",
    "    author VARCHAR(255),\n",
    "    publishedAt DATE,\n",
    "    language VARCHAR(7),\n",
    "    url NVARCHAR(1000),\n",
    "    CONSTRAINT unique_news UNIQUE (source, title, description, author)\n",
    ")\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733305e9",
   "metadata": {},
   "source": [
    "## **<u>Insertar los datos ya limpios en la tabla</u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a639f6",
   "metadata": {},
   "source": [
    "Se procede a verificar si existen datos duplicados entre los que ya estan almacenados en la BD en Redshift, y los que estan por insertarse.\n",
    "Tambien se crea un contador para tener seguimiento(en caso de quererlo) acerca de cuantas filas se dejan de lado, por ser duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49d8a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_count = 0\n",
    "\n",
    "# Verificar si cada fila ya existe antes de insertar los datos\n",
    "for index, row in news_cleaned.iterrows():\n",
    "    cur.execute(f'''\n",
    "        SELECT COUNT(*) FROM {table_name}\n",
    "        WHERE source = %s AND title = %s AND description = %s AND author = %s\n",
    "    ''', (\n",
    "        row['source'],\n",
    "        row['title'],\n",
    "        row['description'],\n",
    "        row['author']\n",
    "    ))\n",
    "    count = cur.fetchone()[0]\n",
    "    if count == 0:\n",
    "        cur.execute(f'''\n",
    "            INSERT INTO {table_name} (source, title, description, author, publishedAt, language, url)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s);\n",
    "        ''', (\n",
    "            row['source'],\n",
    "            row['title'],\n",
    "            row['description'],\n",
    "            row['author'],\n",
    "            row['publishedAt'],\n",
    "            row['language'],\n",
    "            row['url']\n",
    "        ))\n",
    "    else:\n",
    "        duplicates_count += 1\n",
    "\n",
    "print(f\"{duplicates_count} datos duplicados no fueron insertados.\")\n",
    "\n",
    "# Hacer commit de la transacción y cerrar la conexión\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0213b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#En caso de que el codigo para insertar los datos en la tabla de redshift fallen, ejecuto:\n",
    "#cur.execute('ROLLBACK')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa9bbf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
