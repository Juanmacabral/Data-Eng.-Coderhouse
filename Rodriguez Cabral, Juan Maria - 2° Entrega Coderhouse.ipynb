{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5de0a19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#librerias para el tratamiento de la API, transformacion en data frame, y tratamiento de fechas\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "#librerias para conectar la tabla con redshift\n",
    "import psycopg2\n",
    "\n",
    "#libreria para tratar las credenciales\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8191f392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el archivo de credenciales\n",
    "with open('credentials.json') as file:\n",
    "    credentials = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "361da84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parametros de la busqueda\n",
    "\n",
    "query = '''ucrania OR Ukraine OR Ukraine OR Ucraina'''#palabras clave para efectuar la busqueda en API\n",
    "\n",
    "today = datetime.today()\n",
    "yesterday = today - timedelta(days=1) # Obtener la fecha del dia de ayer\n",
    "from_date = yesterday.strftime('%Y-%m-%d')\n",
    "results_limit = 100 \n",
    "api_key = credentials['api_key']\n",
    "languages = ['es', 'en', 'fr', 'it']\n",
    "\n",
    "# Funcion para realizar la conexion a la API\n",
    "def get_articles(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print('Error al conectarse a la API:', e)\n",
    "        return []\n",
    "    else:\n",
    "        data = response.json()\n",
    "        articles = data['articles']\n",
    "        return articles\n",
    "\n",
    "# Crear una lista de diccionarios con los datos de cada artículo.\n",
    "articles_list = []\n",
    "for lang in languages:\n",
    "    url = f\"https://newsapi.org/v2/everything?q={query}&language={lang}&from={from_date}&pageSize={results_limit}&apiKey={api_key}\"\n",
    "    articles = get_articles(url)\n",
    "    for article in articles:\n",
    "        article_data = {\n",
    "            'author': article['author'],\n",
    "            'title': article['title'],\n",
    "            'description': article['description'],\n",
    "            'url': article['url'],\n",
    "            'publishedAt': article['publishedAt'],\n",
    "            'source': article['source']['name'],\n",
    "            'language': lang\n",
    "                        }\n",
    "        articles_list.append(article_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd1fb665",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convertir la lista de diccionarios en un DataFrame\n",
    "df = pd.DataFrame(articles_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7d8900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables y funciones generales\n",
    "#funcion para transformar las siglas de la columna language\n",
    "def transformar_siglas_a_idiomas(df, columna):\n",
    "    siglas_a_idiomas = {\n",
    "        'en': 'english',\n",
    "        'fr': 'french',\n",
    "        'it': 'italian',\n",
    "        'es': 'spanish'\n",
    "    }\n",
    "    df[columna] = df[columna].map(siglas_a_idiomas)\n",
    "    return df\n",
    "\n",
    "#Base para reordenar las columnas\n",
    "column_order = ['source', 'title', 'description','author',  'publishedAt','language', 'url']\n",
    "\n",
    "def limpiar_celdas(columna):\n",
    "    #Excepciones : , . numeros vocoales con tilde /\n",
    "    patron = r\"[^a-zA-ZáéíóúÁÉÍÓÚüÜñÑ.,\\d\\s/]\" #Patrón de expresión regular para encontrar caracteres no deseados\n",
    "    columna = columna.apply(lambda x: re.sub(patron, '', str(x)))  # Aplicar el reemplazo a cada celda de la columna\n",
    "    return columna\n",
    "\n",
    "def eliminar_parentesis(columna):\n",
    "    patron = r'\\((.*?)\\)'  # Patrón de expresión regular para encontrar el texto entre paréntesis\n",
    "    columna = columna.apply(lambda x: re.sub(patron, '', x)).str.strip()\n",
    "    # Eliminar el texto entre paréntesis y eliminar espacios en blanco\n",
    "    return columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51f3cd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_cleaned = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd5455e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limpieza general del DF\n",
    "\n",
    "#algunas filas son iguales entre si, con la excepcion del url. Elimino duplicados sin tener en cuenta el link.\n",
    "news_cleaned = news_cleaned.drop_duplicates(subset=['source', 'title', 'description', 'author', 'publishedAt', 'language'])\n",
    "# Paso a lowercase todas las palabras, a fin de evitar inconsistencias o discrepancias a la hora \n",
    "# de hacer un analisis de los textos\n",
    "news_cleaned = news_cleaned.apply(lambda x: x.astype(str).str.lower())\n",
    "#ejecuto funcion para tranformar las siglas en idiomas en la columa 'languages'\n",
    "news_cleaned = transformar_siglas_a_idiomas(news_cleaned, 'language')\n",
    "#reemplazo los valores vacios por Nan\n",
    "news_cleaned = news_cleaned.replace('', np.nan)\n",
    "#Reemplazo los Nan values (en el caso de la columna 'author', busco el diario que publico la nota)\n",
    "#En el caso de la descripcion sobre la nota, dejo el apartado 'no data'\n",
    "news_cleaned['author'] = news_cleaned['author'].fillna(news_cleaned['source'])\n",
    "news_cleaned['description'] = news_cleaned['description'].fillna('no data')\n",
    "#Reordeno las columnas\n",
    "news_cleaned = news_cleaned.reindex(columns=column_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1b1c27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limpieza especifica por columna\n",
    "\n",
    "#Columna source\n",
    "news_cleaned['source'] = eliminar_parentesis(news_cleaned['source'])\n",
    "\n",
    "#Columna authors\n",
    "#algunas celdas figuran con 'none'. Misma solucion que con los Nan Values, reemplazo el none\n",
    "#por el medio periodistico\n",
    "mask = news_cleaned['author'].eq('none')\n",
    "news_cleaned['author'] = news_cleaned['author'].where(~mask, news_cleaned['source'])\n",
    "#Muchos autores figuran con la siguiente descripcion: la nacion (Carlos pagni). Lo que hago aca\n",
    "#es dejar unicamente los nombres que figuran entre parentesis\n",
    "mask = news_cleaned['author'].str.contains(r'\\([^)]+\\)')\n",
    "news_cleaned.loc[mask, 'author'] = news_cleaned.loc[mask, 'author'].str.extract(r'\\(([^)]+)\\)', expand=False)\n",
    "#Ejecuto funcion para limpiar columna de caracteres no alfabeticos (salvo excepciones)\n",
    "news_cleaned['author'] = limpiar_celdas(news_cleaned['author'])\n",
    "#ajusto casos puntuales mediante replace\n",
    "news_cleaned['author'] = (\n",
    "    news_cleaned['author']\n",
    "    .str.replace('rt en español\\n', 'rt en español')\n",
    "    .str.replace('https//www.facebook.com/bbcnews', 'bbcnews')\n",
    "    .str.replace('rt en español , rt en español', 'rt en español')\n",
    ")\n",
    "\n",
    "#columna publishedAt\n",
    "#elimino caracteres sobrantes, quedando unicamente YYYY-MM-DD\n",
    "news_cleaned['publishedAt'] = news_cleaned['publishedAt'].str[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e04517c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexion con Redshift exitosa\n"
     ]
    }
   ],
   "source": [
    "# Realizar conexion a Redshift mediante psycopg2\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "    host='data-engineer-cluster.cyhh5bfevlmn.us-east-1.redshift.amazonaws.com',\n",
    "    dbname=credentials['db_username'],\n",
    "    user=credentials['db_password'],\n",
    "    password='qjA21aB81Y',\n",
    "    port='5439'\n",
    ")\n",
    "    print(\"Conexion con Redshift exitosa\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Error al conectar con Redshift\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44b1360b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un cursor para ejecutar sentencias SQL\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62449eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar la tabla si ya existe. Ejecutar solo en caso de error\n",
    "#cur.execute('DROP TABLE IF EXISTS news_articles')\n",
    "\n",
    "# Crear una tabla en Redshift, en caso de que no exista\n",
    "table_name = 'news_articles'\n",
    "cur.execute(f'''\n",
    "CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "    id INT IDENTITY(1, 1),\n",
    "    source VARCHAR(255),\n",
    "    title VARCHAR(1000),\n",
    "    description VARCHAR(1000),\n",
    "    author VARCHAR(255),\n",
    "    publishedAt DATE,\n",
    "    language VARCHAR(7),\n",
    "    url NVARCHAR(1000)\n",
    ")\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9cb421f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insertar los datos obtenidos de la API en la tabla\n",
    "for index, row in news_cleaned.iterrows():\n",
    "    cur.execute(f'''\n",
    "        INSERT INTO {table_name} (source, title, description, author, publishedAt, language, url)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s);\n",
    "    ''', (\n",
    "        row['source'],\n",
    "        row['title'],\n",
    "        row['description'],\n",
    "        row['author'],\n",
    "        row['publishedAt'],\n",
    "        row['language'],\n",
    "        row['url']\n",
    "    ))\n",
    "\n",
    "# Hacer commit de la transacción y cerrar la conexión\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0213b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#En caso de que el codigo para insertar los datos en la tabla de redshift fallen, ejecuto:\n",
    "#cur.execute('ROLLBACK')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28e4be1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
